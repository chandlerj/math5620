\documentclass[10pt]{article}
\textheight=9.25in \textwidth=7in \topmargin=-.75in
 \oddsidemargin=-0.25in
\evensidemargin=-0.25in
\usepackage{url}  % The bib file uses this
\usepackage{graphicx} %to import pictures
\usepackage{amsmath, amssymb}
\usepackage{theorem, multicol, color}
\usepackage{gfsartemisia-euler}

\setlength{\intextsep}{5mm} \setlength{\textfloatsep}{5mm}
\setlength{\floatsep}{5mm}
\setlength{\parindent}{0em} % new paragraphs are not indented
\setcounter{MaxMatrixCols}{20}
\usepackage{caption}
\captionsetup[figure]{font=small}


%%%%  SHORTCUT COMMANDS  %%%%
\newcommand{\ds}{\displaystyle}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\arc}{\rightarrow}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\blank}{\underline{\hspace{0.33in}}}
\newcommand{\qand}{\quad and \quad}
\renewcommand{\stirling}[2]{\genfrac{\{}{\}}{0pt}{}{#1}{#2}}
\newcommand{\dydx}{\ds \frac{d y}{d x}}
\newcommand{\ddx}{\ds \frac{d}{d x}}
\newcommand{\dvdx}{\ds \frac{d v}{d x}} 

%%%%  footnote style %%%%

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\pagestyle{empty}

\begin{document}

\begin{flushright}
Chandler Justice - A02313187
\end{flushright}
\noindent \underline{\hspace{3in}}\\
\textbf{January 22, 2024}\\

\textbf{Homework:}\\
For question 4: determine 

\[D_n h(\bar{y}) = c_1 u(x_1) + c_2 u(x_2) + ... + c_n u(x_n)\]
What we will want to do is take the vanderbaun matrix and multiply it by our constants to get a vector of all $0$s and a $1$ 


\textbf{Example:}
\[u''(\bar{x}) = \frac{u(\bar{x} - h) - 2u(\bar{x}) + u( \bar{x} + h)}{h^2} = err\]
\[|E| = |u''(\bar{x}) - \frac{1}{h^2}(u(\bar{x} + h) - 2u(\bar{x}) + u(\bar{x} - h) |\]
\[ = |u''(\bar{x}) - \frac{1}{h^2} \{(u(\bar{x}) + hu'(\bar{x}) + \frac{1}{2} h^2 u''(\bar{x}) + \frac{h^3}{6} u'''(\bar{x}) + \frac{h^4}{24} u''''(\zeta_1))\]
\[-2u (u(\bar{x}) - hu(\bar{x}) + \frac{1}{2}h^2 u''(\bar{x}) - \frac{1}{6}h^3 (\bar{x}) + \frac{h^4}{24}u''''(\zeta_2))\}\]

\[= |u'' - \frac{1}{h^2} \{h^2 u''(\bar{x}) + \frac{1}{24}h^4 u''''(\zeta_3)\}|\]

\textbf{Heat Equation:} Heat equation:

\[\frac{\partial u}{\partial t} = \frac{\partial}{\partial k} k(x) \frac{\partial u}{\partial x} + t (x,t)\]

If we assume $k$ is continuous

\[\frac{\partial u}{\partial t} = K \frac{\partial^2 u}{\partial x^2} + t(x,t)\]

steady state = 1 $\Rightarrow$ $\frac{\partial u}{\partial t}  = 0$ $\Rightarrow$ $K u'' + t(x,t) = 0$\\

\[
    \begin{cases}
        u'' = f(x)\\
        u(0) = \alpha\\
        u(1) = \beta\\
    \end{cases}
\]

Exact solution:
\[\int u''(x) dx = \int f(x) dx = g(x) + c\]
\[u' = g(x) + c_1\]
\[u = \int g(x) + c_1 x + c_2\]

We could decide to get an approximation at discrete points in the domain. Lets our domain be $[0,1]$.\\

So we will use equally spaced (for now) points in $[0,1]$, say $m + 2$ points. Then
\[h = \frac{1}{m + 1} \Rightarrow \{u_0, u_1, ..., u_m, u_{m+1}\} \quad \text{(size = $m + 2$)}\]
\[x_j = j * h\]
\[u_0 = \alpha\]
\[u_{m+1} = \beta\]
our $u_0, u_{m+1}$ variables will be exact values.\\
$u'' = f(x)$ at $x_0, x_1, ... x_n$\\

\[D^2 u_j = \frac{u_{j-1} 2u_j + u_{j+1}}{h^2}\]
so
\[\frac{1}{h^2}(u_{j-1} - 2u_j + u_{j+1}) \approx f(x_j)\]
for $j = 0,1,2,...,m+1$\\
\[\begin{matrix}
    j = 0 & u_0 = \alpha\\
    j = 1 & \frac{1}{h^2}(u_0 - 2u_1 + u_2) = f(x_1)\\
    j = 2 &  \frac{1}{h^2}(u_1 - 2u_2 + u_3) = f(x_1)\\
    ... & ...\\
    j = m &  \frac{1}{h^2}(u_{m - 1} - 2u_m + u_{m + 1}) = f(x_m)\\
\end{matrix}
\]

$$
\begin{bmatrix}
    1 & 0 & ... & 0 & 0\\
    -\frac{1}{h^2} & -\frac{2}{h^2} & ... & 0\\
    0 &  -\frac{1}{h^2} & -\frac{2}{h^2} & ... & 0\\
\end{bmatrix}
\begin{bmatrix}
    u_0\\
    u_1\\
    u_n\\
\end{bmatrix}
=
\begin{bmatrix}
    \alpha\\
    f_1\\
    f_2\\
    f_n\\
    \beta
\end{bmatrix}
$$

Then we get

$$
\begin{bmatrix}
    \frac{\alpha}{n^2}\\
    0\\
    ...\\
    0\\
    \frac{\beta}{n^2}
\end{bmatrix}
\begin{bmatrix}
    -2 & 1 & 0 & ... & 0 & 0\\
    1 & -2 & 1 & 0 & ... & 0\\
    0 & 1 & -2 & 1 & 0 & ...\\
    ... & ... & ... & ...\\
\end{bmatrix}
=
\begin{bmatrix}
    f_1\\
    f_2\\
    ...\\
    f_m
\end{bmatrix}
$$

To summarize we get a matrix $A$ that we multiply by a vector $U$ to get the vector of functions $F$.\\


We need to be able to define and interpret an error. Suppose we define
$$
\hat{U} =
\begin{bmatrix}
    u(x_1)\\
    u(x_2)\\
    ...
    u(x_m)
\end{bmatrix}
, \quad U = 
\begin{bmatrix}
    u_1\\
    u_2\\
    ...
    u_n
\end{bmatrix}
\in \R
$$

Then
\[E = U - \hat{U}\]

\textbf{Definition:} \textit{Vector norm - } Any function || * ||:$\R^m \in \R$ is a norm if
\begin{enumerate}
    \item for any vector $v \in \R^m$, $||v|| \geq 0$ and $||v|| = 0$ iff $x=0$
    \item for any vector $v \in \R^m$ and scaler $a$, $||av|| = |a| * ||v||$
    \item for $u, v \in \R^m$, $||u + v|| \leq ||u|| + ||v||$
    \end{enumerate}

\textit{The norms:}
\begin{itemize}
    \item $||E||_\infty = max_{1 \leq j \leq m} |u_j - u(xj)|$
    \item $||E||_1 = h \sum^m_{j=1} |u_j - u(x_j) |$
    \item $||E||_2 = (h \sum^n_{j = 1}|u - u(x_j)|^2)^\frac{1}{2}$
    \end{itemize}

\textbf{January 24, 2024}\\

\textbf{Example:} 2 point boundary problem\\

\[\begin{cases}
    u'' = f(x) \quad 0 < x < 1\\
    u(0) = \alpha\\
    u(1) = \beta
  \end{cases}
\]

Lets employ a centered difference

\[u''(x) \approx \frac{u(x-h) - 2u(x) + u(x + h)}{h^2}\]

If we want the error we can compute

\[|e(x)| = |u''(x) - \frac{u(x-h) -2u(x) + u(x+h)}{h^2} | \leq O(h^2)\]

We want to approximate $u$ at discrete points. To do this, we need to pick points over an interval where each point is $h$ apart from the next, $h = 1/(m + 1)$, $x_j = j*h$, and $j = \{1,2,3,4,...,m+1\}$.\\

\textbf{Definition:}
\[u(x_j) = u_j \rightarrow u''(x_j) = \frac{u_{j-1} - 2u_j + u_{j+1}}{h^2} \quad j = 1,2,...m\]

\[\rightarrow \frac{u_{j-1} - 2u_j + u_{j + 1}}{h^2}\]
Working through these computations we get $A U = F$
$$
\begin{bmatrix}
    -2 & 1 & 0 & ... & 0\\
    1 & -2 & 1 & ... & 0\\
    ... & ... & ... & ... & ...\\
    0 & 0 & ... & 1 & -2\\ 
\end{bmatrix}
\begin{bmatrix}
    u_1\\
    u_2\\
    ...\\
    u_m\\
\end{bmatrix}
= 
\begin{bmatrix}
    f_1 - \alpha/h^2\\
    f_2\\
    ...\\
    f_{m-1}\\
    f_{m-1} - Ch^2
\end{bmatrix}
$$

In general, we can use the classic $Ax = b$ method from linear algebra to solve these systems of equations. We can do this in code with
\begin{verbatim}
    for k = 1, m = 1
        for i = k + 1, m
            factor = a[i][k] 
            for j = k + 1, m
                a[i][j] = a[i][j] - factor * a[i][j]
            end
            b[i] = b[i] - factor * b[j]
        end
    end
\end{verbatim}

For a tridiagonal matrix we can perform elimination via
\begin{verbatim}
    for k=1, m=1
        factor = a[k + 1][k] / a[k][k]
        a[k + 1][k+1] = a[k+1][k+1] - factor * a[k][k+1]
        b[k+1] = b[k+1] - factor * b[k]
    end
\end{verbatim}
\newpage
After running this algorithm on a tridiagonal matrix, it will result in a matrix with all zeros except for along the two center diagonal entries where $a'_{i, i}, a'_{i + 1, i}$. From here, we can to our $AU = F$ compuattion, where
\[u_m = (b'_m/a'_{m}{m})\]
\[u_{m-1} = (b'_{m=1} - a_{m-1, m-1} a_m)/a_{m-1, m-1}\]
\[u_k = (b'_k - a'_{k, m+1} * u_{m+1}) / a'_{k,k}\]

Putting all this shit together is referred to as the \textbf{Thomas algorithm}.\\

Lets say we don't want to waste computation time and memory storing a bunch of zeros. We can avoid this by decomposing the nonzero elements into vectors where
\[A = 
    \begin{cases}
        a_d = \text{main diagonal}\\
    a_{l1} = \text{1st ??? diagonal}\\
    a_{s1} = \text{1nd super diagonal}\\
    \end{cases}
\]
We can implement this in code using

\begin{verbatim}
    # forward elimination 
    for k=1, m=1
        factor = al1[k] / ad[k]
        ad[k+1][k+1] = ad[k+1][k+1] - factor * as1[k]
        b[k+1] = b[k+1] - factor * b[k]
    end

    # back substitution
    u(m) = k[m] / ad[m]
    for k = m-1, 1
        u[k] = (b[k] - as1[k] * u[k+1]) / ad[k])
    end
\end{verbatim}

\textbf{January 26, 2024}\\

\textbf{Example:} 
\[\frac{\partial u}{\partial t} = \frac{\partial}{\partial x} h \frac{\partial u}{\partial x} + t(x, t)\]

simplify to

\[
    \begin{cases}
        u''(x) = f(x)\\
        u(0) = \alpha\\
        u(1) = \beta
    \end{cases}
\]

which gives us

\[\frac{u(x_j - h) - 2u(x_j) + u(x_j + h)}{h^2} \approx f(x_j)\]
we will rerepsent this as
\[\frac{U_{j-2} - 2U_j + U_{j+1}}{h^2} = f(x_j) = F_j\]

We can compose this expression into a matrix with the shape

\[ A =
    \begin{bmatrix}
        -2 & 1 & ... & 0\\
        1 & -2 & ... & 0\\
        0 & ... & 1 & -2\\
    \end{bmatrix}
    ,U = 
    \begin{bmatrix}
        U_1\\
        U_2\\
        ...\\
        U_N\\
    \end{bmatrix}
    ,F =
    \begin{bmatrix}
        f_1 - \alpha / h^2\\
        f_2 \\
        ...\\
        f_m - \beta/h^2
    \end{bmatrix}
\]
Using this form allows us to use the \textit{Thomas algorithm}.\\
\[\Rightarrow U = A^{-1} * F\]

\textbf{Errors:}
\begin{align*}
    ||\hat{U} - U|| &\leq Ch^2\\
    ||\hat{U} - U||_\infty &= max_{l \leq j \leq m} |\hat{u}_j - u_j\\
    ||\hat{u} - U||_1 &= h * \sum_{j=1}^m |\hat{u}_j = u_j|\\
    [||\hat{u} - U||_2 &= (h \sum_{j=1}^m (\hat{u}_j - u_j)^2)^{1/2} \rightarrow ||\hat{u} = U||_2^2\\
\end{align*}

\textit{We talked about jacobi iteration which we covered in substantial detail last semester, so look at those notes; specifically notes back from nov 2023}

\textbf{N-n-n-new shit}\\
\begin{align*}
x^{k+1} &= D^{-1}(b - (L + D - D + U)x^{(k)}\\
        &= D^{-1}(b - (L+D+U)x^{(k)})\\
        &= D^{-1}(b - Ax^{(k)}) + x^{(k)}\\
        &= D^{-1}r^{(k)} + x^{(k)}\\
x^{(k+1)} &= x^{(k)} + D^{-1} r^{(k)}\\
\end{align*}

$Du$: if 
\[Ax = b\]
We derive the residual vector to be
\[r = b - Ax\]
If $r = 0 \rightarrow 0 = b - Ax$
\[\rightarrow Ax = b\]

We have two methods for solving $AU = F$
\begin{enumerate}
    \item Thomas algoritm (stupid)
    \item Jacobi Iteration (also stupid)
\end{enumerate}


\underline{jacobi}\\
\textit{input}  $A, b, x^0$\\
\textit{initialize}  $r^0 = b - Ax^0$\\
\textit{loop:}
\begin{verbatim}
    x_k = D_-1 * r_0 + x_0
        = x_0 + D_-1 * r_0
    r_1 = b - A * x_1
    test: ||r_1|| -> 0
\end{verbatim}
\newpage
\underline{Full matrix mutliplication algorithm}\\
$Ax \rightarrow y$
\begin{verbatim}
    for i in range(m):
        y[i] = 0.0
        for j in range(m):
            y[i] = y[i] + a[i][j] * x[j]
        end
    end
\end{verbatim}
We could prolly make this a bit faster
\begin{verbatim}
    for i in range(m):
        sum = 0.0
        for j in range(i-1, i+2):
            sum = sum + A[i][j] + x[j]
        end
        y[i] = sum
    end
\end{verbatim}

\textbf{January 29, 2024}\\

We have a way to approximately solve these systems
\[Au = F \Rightarrow u = A^{-1}F\]
Use back-substituion to find $A^{-1}$ or Jacobi iteration.\\

\underline{Methods:}
\begin{itemize}
    \item Gauss-Elimination with back substitution
    \item jacobi interation
\end{itemize}

We want to figure out as $h \to 0$, what do we converge to?\\

\textbf{Local Truncation Error}\\
Our finite difference method is
\[\begin{cases}
\frac{U_{j-1} - 2U_j + U_{j+1}}{h^2} = f(x_j)\\
U_0 = \alpha\\
u_{m+1} = \beta\\
\end{cases}\]
The local truncation error is computed by substituting the exact solution $U(x_j)$ explicitily with taylor series.
\begin{align*}
    \tau_j = \tau(x_j) &= \frac{1}{h^2}(u(x_{j-1} - 2u(x_j) + u(x_{j+1}) - f(x_j)\\
                       &= [u''(x_j) + \frac{1}{12}h^2u''''(x_j) + O(h^4)] - f(x_j)\\
                       &= \frac{1}{12} h^2 u''''(x_j) + O(h^4)\\
                       &\leq Ch^2
\end{align*}
We do not know $u''''(x_j)$, but we can assume $u''''(x_j)$ is independent of $h$. There will be $m$ LTEs (local truncation errors) we are concerned with.\\
\newpage
\textbf{Definition:} 
\[\tau = A \hat{U} - F =
\begin{bmatrix}
\tau_1\\
\tau_2\\
...\\
\tau_3\\
\end{bmatrix}
\Rightarrow A\hat{U} = F + \tau\]

\textbf{Definition:} Global error
\[E = U - \hat{U} \rightarrow ||E|| = ||U - \hat{U}||\]

\[\begin{cases}
    A\hat{U} - F = \tau\\
    AU - F = 0
\end{cases}\]
\[(AU - F) - (A\hat{U} - F) = -\tau\]
\[A(U - \hat{U}) = \tau\]
\[AE = -\tau\]
\[\frac{E_{j+1} - 2E_j + E_{j-1}}{h^2} = -tau_j\]
\[E_0 = 0 \Rightarrow E_{m+1} = 0\]
Given $\tau_j$ in $O(h^2)$ we want \underline{$||E|| \leq Ch^2$}.\\
We can write a continous analogue to the ODE:
\[\begin{cases}
    e''(x) = -\tau(x)\\
    e(x) = 0\\
    e(1) = 0
\end{cases} \Rightarrow \tau(x) \approx \frac{1}{12} h^2u''''(x)\]
\[e'' = \int^1_0 \frac{1}{12}h^2 u''''(x) dx \Rightarrow e(x) = \frac{2}{12}h^2 u''\]

\underline{Stability}\\

given
\[A^h E^h = -\tau^h \quad \text{where }h \text{ is the width of the mesh}\]
\begin{align*}
E^h &= -(A^h)^{-1} \tau^h\\
||E^h|| &= ||(A^h)^{-1} \tau^h \\
        &\leq ||(A^h)^{-1}|| ||\tau^h|| \rightarrow ||E^h|| \leq ||(A^h)^{-1}|| ||\tau_j||
\end{align*}

\textbf{Definition:} Suppose a finite difference method for a BVP give a sequence of matrix equations of the form $A^h U^h = F^h$, where $h$ is the mesh width. We say the method is \underline{stable} if $(A^h)^{-1}$ exists for all $h$ sufficiently small $(h < h_0)$ and a constant exists independent of $h$ such that
\[||(A^h)^{-1}|| \leq C\]

If $(LTE \to 0$ as $h \to 0)$ + stability $\Rightarrow$ \underline{Convergence}.\\

\textbf{Matrix Norms.}

\textbf{Defintition:} A \underline{norm} on a vector space is a function that satisifes three properties
\begin{enumerate}
    \item $||x|| > 0$ for all x in the V.S.
    \item $||\alpha x|| = |\alpha| ||x||$ for all x and $\alpha \in \R$
    \item $||x | g || \leq ||x|| + ||g||$
    \item $||A B|| \leq ||A|| ||B||$
\end{enumerate}

If the norm satisfies (4), the norm is said to be a consistent norm.

\textbf{Example:} $||(A^h)^{-1} \tau|| \leq ||(A^h)^{-1}|| ||\tau||$
We will want individual norms:
\begin{align*}
    ||A||_1 &= max_{x \in \R_m} \frac{||Ax||}{AxU}\\
||A||_p &= max \frac{||A_x||_p}{||x||_p}\\
    ||A||_\infty &= max_{x \in \R_m} \frac{||Ax||_\infty}{||x||_\infty}\\
\end{align*}
\noindent \underline{\hspace{3in}}\\

\end{document}

